{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 方策評価ノートブック\n",
    "\n",
    "不確実性パラメタ ω を10等分した各点で、各手法（M2TD3, DR(TD3), RARL）の方策を30エピソード評価し、平均・標準誤差・最悪平均を可視化します。\n",
    "\n",
    "参考: [Tanabe et al., 2022](https://arxiv.org/pdf/2211.03413) の図5・図6\n"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. 設定\n",
    "import os\n",
    "import sys\n",
    "import glob\n",
    "import importlib.util\n",
    "from pathlib import Path\n",
    "from typing import Dict, List, Tuple, Optional\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import gymnasium as gym\n",
    "import torch\n",
    "import rrls  # RRLS環境の登録\n",
    "\n",
    "# 実験設定\n",
    "env_name = \"HalfCheetah\"  # \"Ant\", \"HalfCheetah\", \"Hopper\", \"InvertedPendulum\", \"Walker\", \"Walker2d\"\n",
    "nb_dim = 1  # 不確実性次元: 1, 2, または 3\n",
    "nb_mesh = 10  # 各次元の分割数（10等分）\n",
    "seeds = 30  # 各ωでのエピソード数\n",
    "max_steps = 1000  # 1エピソードの最大ステップ数\n",
    "device = \"cpu\"\n",
    "\n",
    "# TCRMDP/srcのパス（M2TD3用）\n",
    "# リポジトリルート（ノートブック実行時はカレントをリポジトリルートに cd すること。環境変数 WORKSPACE_ROOT で上書き可）\n",
    "workspace_root = Path(os.environ[\"WORKSPACE_ROOT\"]) if os.environ.get(\"WORKSPACE_ROOT\") else Path.cwd()\n",
    "tcrmdp_src = str(workspace_root / \"TCRMDP\" / \"src\")\n",
    "\n",
    "# 方策パスの指定（globパターン可、複数指定可）\n",
    "# 相対パスはworkspace_rootを基準に解決されます\n",
    "method_to_globs: Dict[str, List[str]] = {\n",
    "    \"M2TD3\": [\"exp/HC/HC_dim1_M2TD3_seed*/**/policies/policy-*.pth\"],\n",
    "    \"DR\": [\"exp/HC/HC_dim1_DR_seed*/agent.pth\"],\n",
    "    \"soft-omega-M2TD3\": [\"exp/HC/HC_dim1_soft-omega-M2TD3_seed*/**/policies/policy-*.pth\"],\n",
    "    # 学習済み方策のパスを上記 glob で解決するか、必要に応じて追加\n",
    "}\n",
    "\n",
    "# 結果キャッシュのパス（Noneで無効化）\n",
    "cache_path = None  # 例: \"cache/evaluation_results.npz\"\n",
    "\n",
    "print(f\"環境: {env_name}, 次元: {nb_dim}D, メッシュ: {nb_mesh}, エピソード数: {seeds}, デバイス: {device}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2. ユーティリティ読み込み（scriptsから関数を動的インポート）\n",
    "\n",
    "def load_module_from_file(filepath: str, module_name: str):\n",
    "    \"\"\"ファイルパスからモジュールを動的に読み込む\"\"\"\n",
    "    spec = importlib.util.spec_from_file_location(module_name, filepath)\n",
    "    if spec is None or spec.loader is None:\n",
    "        raise ImportError(f\"Failed to load module from {filepath}\")\n",
    "    module = importlib.util.module_from_spec(spec)\n",
    "    sys.path.insert(0, str(Path(filepath).parent))\n",
    "    spec.loader.exec_module(module)\n",
    "    return module\n",
    "\n",
    "# scriptsディレクトリのパス\n",
    "scripts_dir = workspace_root / \"scripts\"\n",
    "eval_m2td3_path = str(scripts_dir / \"rrls_light_eval.py\")\n",
    "eval_td3_path = str(scripts_dir / \"rrls_light_eval_td3.py\")\n",
    "\n",
    "# モジュールを読み込み\n",
    "eval_m2td3 = load_module_from_file(eval_m2td3_path, \"rrls_light_eval\")\n",
    "eval_td3 = load_module_from_file(eval_td3_path, \"rrls_light_eval_td3\")\n",
    "\n",
    "# 必要な関数を取得\n",
    "rrls_components = eval_m2td3.rrls_components\n",
    "rollout_return = eval_m2td3.rollout_return\n",
    "build_agent_m2td3 = eval_m2td3.build_agent\n",
    "build_agent_td3 = eval_td3.build_agent\n",
    "env_id_from_name = eval_m2td3.env_id_from_name\n",
    "resolve_policy_path = eval_m2td3.resolve_policy_path\n",
    "\n",
    "\n",
    "# --- M2SAC 用の軽量ビルド関数 -----------------------------\n",
    "# M2SAC の policy-*.pth（GaussianPolicyNetwork の state_dict）を\n",
    "# 評価用に読み込むためのラッパをノートブック側に実装する。\n",
    "\n",
    "import importlib.util as _importlib_util\n",
    "from types import ModuleType as _ModuleType\n",
    "\n",
    "\n",
    "class _M2SACAgentWrapper:\n",
    "    \"\"\"GaussianPolicyNetwork をラップして select_action を提供する簡易エージェント\"\"\"\n",
    "\n",
    "    def __init__(self, policy: torch.nn.Module, device: torch.device, obs_dim: int):\n",
    "        self.policy = policy.to(device)\n",
    "        self.device = device\n",
    "        self.obs_dim = obs_dim\n",
    "        self.policy.eval()\n",
    "\n",
    "    @torch.no_grad()\n",
    "    def select_action(self, obs: np.ndarray, use_random: bool = False) -> np.ndarray:\n",
    "        state_tensor = torch.tensor(obs, dtype=torch.float32, device=self.device).view(\n",
    "            -1, self.obs_dim\n",
    "        )\n",
    "        action = self.policy(state_tensor)\n",
    "        return action.squeeze(0).detach().cpu().numpy()\n",
    "\n",
    "\n",
    "def _infer_m2sac_arch(path: str) -> tuple[int, int]:\n",
    "    \"\"\"M2SAC の policy-*.pth から隠れ層数とユニット数を推定\"\"\"\n",
    "    sd = torch.load(path, map_location=\"cpu\")\n",
    "    # hidden layer width = out_features of input_layer\n",
    "    hidden_layer = (\n",
    "        int(sd[\"input_layer.weight\"].shape[0])\n",
    "        if \"input_layer.weight\" in sd\n",
    "        else 256\n",
    "    )\n",
    "    # count hidden layers by scanning keys like hidden_layers.<idx>.weight\n",
    "    max_idx = -1\n",
    "    prefix = \"hidden_layers.\"\n",
    "    suffix = \".weight\"\n",
    "    for k in sd.keys():\n",
    "        if k.startswith(prefix) and k.endswith(suffix):\n",
    "            try:\n",
    "                idx = int(k[len(prefix) :].split(\".\")[0])\n",
    "                if idx > max_idx:\n",
    "                    max_idx = idx\n",
    "            except Exception:\n",
    "                continue\n",
    "    hidden_num = max_idx + 1 if max_idx >= 0 else 0\n",
    "    return hidden_num, hidden_layer\n",
    "\n",
    "\n",
    "def build_agent_m2sac(policy_path: str, env: gym.Env, device: str, tcrmdp_src: str):\n",
    "    \"\"\"M2SAC の policy-*.pth から評価用エージェントを構築する\"\"\"\n",
    "    if tcrmdp_src is None:\n",
    "        raise ValueError(\"tcrmdp_src must be provided to locate m2td3/utils.py\")\n",
    "\n",
    "    tcrmdp_src_resolved = os.path.expanduser(os.path.expandvars(tcrmdp_src))\n",
    "    utils_path = os.path.join(tcrmdp_src_resolved, \"m2td3\", \"utils.py\")\n",
    "    if not os.path.exists(utils_path):\n",
    "        raise FileNotFoundError(f\"m2td3/utils.py not found: {utils_path}\")\n",
    "\n",
    "    spec = _importlib_util.spec_from_file_location(\"m2td3_utils_for_m2sac\", utils_path)\n",
    "    if spec is None or spec.loader is None:\n",
    "        raise ImportError(f\"Failed to load module from {utils_path}\")\n",
    "    module: _ModuleType = _importlib_util.module_from_spec(spec)\n",
    "    if tcrmdp_src_resolved not in sys.path:\n",
    "        sys.path.insert(0, tcrmdp_src_resolved)\n",
    "    spec.loader.exec_module(module)  # type: ignore[attr-defined]\n",
    "    GaussianPolicyNetwork = getattr(module, \"GaussianPolicyNetwork\")\n",
    "\n",
    "    obs_dim = int(np.prod(env.observation_space.shape))\n",
    "    act_dim = int(np.prod(env.action_space.shape))\n",
    "    hidden_num, hidden_layer = _infer_m2sac_arch(policy_path)\n",
    "\n",
    "    device_t = torch.device(device if torch.cuda.is_available() else \"cpu\")\n",
    "    policy = GaussianPolicyNetwork(\n",
    "        state_dim=obs_dim,\n",
    "        action_dim=act_dim,\n",
    "        hidden_num=hidden_num,\n",
    "        hidden_layer=hidden_layer,\n",
    "        max_action=env.action_space.high[0],\n",
    "        device=device_t,\n",
    "    ).to(device_t)\n",
    "\n",
    "    state_dict = torch.load(policy_path, map_location=\"cpu\")\n",
    "    policy.load_state_dict(state_dict)\n",
    "    policy.to(device_t)\n",
    "\n",
    "    return _M2SACAgentWrapper(policy=policy, device=device_t, obs_dim=obs_dim)\n",
    "\n",
    "\n",
    "# --- Ablation(TD3) 用: actor の state_dict だけを保存した policy-*.pth を読む ----\n",
    "class _TD3ActorOnlyWrapper:\n",
    "    def __init__(self, actor: torch.nn.Module, device: torch.device, obs_dim: int):\n",
    "        self.actor = actor.to(device)\n",
    "        self.device = device\n",
    "        self.obs_dim = obs_dim\n",
    "        self.actor.eval()\n",
    "\n",
    "    @torch.no_grad()\n",
    "    def select_action(self, obs: np.ndarray, use_random: bool = False) -> np.ndarray:\n",
    "        state_tensor = torch.tensor(obs, dtype=torch.float32, device=self.device).view(\n",
    "            -1, self.obs_dim\n",
    "        )\n",
    "        action = self.actor(state_tensor)\n",
    "        return action.squeeze(0).detach().cpu().numpy()\n",
    "\n",
    "\n",
    "def build_agent_td3_actor_only(policy_path: str, env: gym.Env, device: str, tcrmdp_src: str):\n",
    "    \"\"\"Ablation TD3 の policy-*.pth（Actorのstate_dict）から評価用エージェントを構築する\"\"\"\n",
    "    if tcrmdp_src is None:\n",
    "        raise ValueError(\"tcrmdp_src must be provided to locate td3/models.py\")\n",
    "\n",
    "    tcrmdp_src_resolved = os.path.expanduser(os.path.expandvars(tcrmdp_src))\n",
    "    models_path = os.path.join(tcrmdp_src_resolved, \"td3\", \"models.py\")\n",
    "    if not os.path.exists(models_path):\n",
    "        raise FileNotFoundError(f\"td3/models.py not found: {models_path}\")\n",
    "\n",
    "    spec = _importlib_util.spec_from_file_location(\"td3_models_for_ablation\", models_path)\n",
    "    if spec is None or spec.loader is None:\n",
    "        raise ImportError(f\"Failed to load module from {models_path}\")\n",
    "    module: _ModuleType = _importlib_util.module_from_spec(spec)\n",
    "    if tcrmdp_src_resolved not in sys.path:\n",
    "        sys.path.insert(0, tcrmdp_src_resolved)\n",
    "    spec.loader.exec_module(module)  # type: ignore[attr-defined]\n",
    "\n",
    "    Actor = getattr(module, \"Actor\")\n",
    "\n",
    "    obs_dim = int(np.prod(env.observation_space.shape))\n",
    "    device_t = torch.device(device if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "    actor = Actor(observation_dim=obs_dim, action_space=env.action_space).to(device_t)\n",
    "\n",
    "    ckpt = torch.load(policy_path, map_location=\"cpu\")\n",
    "    # 旧形式（agent.pth 等）で {\"actor\": ...} の場合にも対応\n",
    "    if isinstance(ckpt, dict) and \"actor\" in ckpt:\n",
    "        ckpt = ckpt[\"actor\"]\n",
    "    actor.load_state_dict(ckpt)\n",
    "    actor.to(device_t)\n",
    "\n",
    "    return _TD3ActorOnlyWrapper(actor=actor, device=device_t, obs_dim=obs_dim)\n",
    "\n",
    "\n",
    "# Ablation SAC / DR-SAC は policy の形式が M2SAC と同じ（GaussianPolicyNetwork state_dict）なので流用\n",
    "build_agent_ablation_sac = build_agent_m2sac\n",
    "build_agent_ablation_dr_sac = build_agent_m2sac\n",
    "\n",
    "print(\"✓ ユーティリティ関数の読み込み完了 (M2SAC + Ablation対応)\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# フォールバック付き rrls_components を定義し、上書き\n",
    "def rrls_components(env_name: str, nb_dim: int):\n",
    "    import importlib\n",
    "    from rrls.evaluate import generate_evaluation_set\n",
    "\n",
    "    def pick_module(candidates):\n",
    "        last_err = None\n",
    "        for mod in candidates:\n",
    "            try:\n",
    "                return importlib.import_module(f\"rrls.envs.{mod}\")\n",
    "            except Exception as e:\n",
    "                last_err = e\n",
    "        raise last_err\n",
    "\n",
    "    # Envごとの候補モジュール名（左から順に試す）\n",
    "    if env_name == \"Ant\":\n",
    "        mod = pick_module([\"ant\"])\n",
    "        ParamsBound = getattr(mod, \"AntParamsBound\")\n",
    "        ModifiedEnv = getattr(mod, \"RobustAnt\")\n",
    "    elif env_name == \"HalfCheetah\":\n",
    "        mod = pick_module([\"halfcheetah\", \"half_cheetah\"])\n",
    "        ParamsBound = getattr(mod, \"HalfCheetahParamsBound\")\n",
    "        ModifiedEnv = getattr(mod, \"RobustHalfCheetah\")\n",
    "    elif env_name == \"Hopper\":\n",
    "        mod = pick_module([\"hopper\"])\n",
    "        ParamsBound = getattr(mod, \"HopperParamsBound\")\n",
    "        ModifiedEnv = getattr(mod, \"RobustHopper\")\n",
    "    elif env_name == \"HumanoidStandup\":\n",
    "        mod = pick_module([\"humanoidstandup\", \"humanoid\"])\n",
    "        ParamsBound = getattr(mod, \"HumanoidStandupParamsBound\")\n",
    "        # クラス名のバリエーションに対応\n",
    "        ModifiedEnv = getattr(mod, \"RobustHumanoidStandup\", getattr(mod, \"RobustHumanoidStandUp\"))\n",
    "    elif env_name == \"InvertedPendulum\":\n",
    "        mod = pick_module([\"invertedpendulum\", \"pendulum\"])\n",
    "        ParamsBound = getattr(mod, \"InvertedPendulumParamsBound\")\n",
    "        ModifiedEnv = getattr(mod, \"RobustInvertedPendulum\")\n",
    "    elif env_name in (\"Walker\", \"Walker2d\"):\n",
    "        mod = pick_module([\"walker2d\", \"walker\"])\n",
    "        # Walker2d系で公開名が異なる場合に対応\n",
    "        ParamsBound = getattr(mod, \"Walker2dParamsBound\", getattr(mod, \"WalkerParamsBound\"))\n",
    "        ModifiedEnv = getattr(mod, \"RobustWalker2d\", getattr(mod, \"RobustWalker\"))\n",
    "    else:\n",
    "        raise ValueError(f\"Unsupported env_name: {env_name}\")\n",
    "\n",
    "    # 次元に応じて境界を選択\n",
    "    if nb_dim == 3:\n",
    "        param_bounds = ParamsBound.THREE_DIM.value\n",
    "    elif nb_dim == 2:\n",
    "        param_bounds = ParamsBound.TWO_DIM.value\n",
    "    else:\n",
    "        # 1次元のときだけ、HalfCheetah の worldfriction 上限を 4.0 に揃える\n",
    "        base = dict(ParamsBound.ONE_DIM.value)\n",
    "        if env_name == \"HalfCheetah\" and \"worldfriction\" in base:\n",
    "            low, _ = base[\"worldfriction\"]\n",
    "            base[\"worldfriction\"] = [low, 4.0]\n",
    "        param_bounds = base\n",
    "\n",
    "    return ModifiedEnv, param_bounds, generate_evaluation_set\n",
    "\n",
    "ModifiedEnv, param_bounds, generate_evaluation_set = rrls_components(env_name, nb_dim)\n",
    "print(\"\\u2713 ModifiedEnv と param_bounds を再取得\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3+. 含端linspaceで評価用環境を再生成（終点を必ず含む）\n",
    "from itertools import product\n",
    "\n",
    "# 既存の ModifiedEnv, param_bounds, nb_mesh を利用\n",
    "\n",
    "def generate_evaluation_set_inclusive(modified_env, param_bounds: dict, nb_mesh_dim: int):\n",
    "    keys = list(param_bounds.keys())\n",
    "    grids = [\n",
    "        np.linspace(v[0], v[1], num=nb_mesh_dim, endpoint=True).tolist()\n",
    "        for k, v in param_bounds.items()\n",
    "    ]\n",
    "    envs = []\n",
    "    for vals in product(*grids):\n",
    "        params = {k: float(val) for k, val in zip(keys, vals)}\n",
    "        envs.append(modified_env(**params))\n",
    "    return envs\n",
    "\n",
    "# 置き換え\n",
    "env_set = generate_evaluation_set_inclusive(ModifiedEnv, param_bounds, nb_mesh)\n",
    "num_envs = len(env_set)\n",
    "print(f\"[inclusive] 環境数: {num_envs}\")\n",
    "\n",
    "# ω値を再抽出（一次元想定）\n",
    "omega_vals = []\n",
    "for env in env_set:\n",
    "    try:\n",
    "        if hasattr(env, \"get_params\") and callable(getattr(env, \"get_params\")):\n",
    "            params = env.get_params()\n",
    "            key0 = list(param_bounds.keys())[0]\n",
    "            omega_vals.append(float(params[key0]))\n",
    "        else:\n",
    "            omega_vals.append(len(omega_vals))\n",
    "    except Exception:\n",
    "        omega_vals.append(len(omega_vals))\n",
    "\n",
    "omega_vals = np.array(omega_vals)\n",
    "if len(set(omega_vals)) == len(omega_vals):\n",
    "    idx = np.argsort(omega_vals)\n",
    "    omega_vals = omega_vals[idx]\n",
    "    env_set = [env_set[i] for i in idx]\n",
    "else:\n",
    "    omega_vals = np.arange(num_envs)\n",
    "\n",
    "print(f\"[inclusive] ω値の範囲: [{omega_vals.min():.3f}, {omega_vals.max():.3f}]\")\n",
    "print(f\"[inclusive] ω値: {omega_vals[:5]} ... {omega_vals[-5:]}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 4. 単一方策の評価関数\n",
    "\n",
    "def evaluate_policy(\n",
    "    policy_path: str,\n",
    "    method: str,  # \"M2TD3\", \"soft-actor-M2TD3\", \"DR\", \"RARL\"\n",
    "    env_set: List[gym.Env],\n",
    "    omega_vals: np.ndarray,\n",
    "    seeds: int,\n",
    "    max_steps: int,\n",
    "    device: str,\n",
    "    tcrmdp_src: str,\n",
    ") -> Tuple[np.ndarray, np.ndarray, np.ndarray]:\n",
    "    \"\"\"\n",
    "    単一方策を全ω×seedsで評価\n",
    "    \n",
    "    Returns:\n",
    "        mean_per_env: 各ωでの平均報酬 (len=nb_mesh)\n",
    "        sem_per_env: 各ωでの標準誤差 (len=nb_mesh)\n",
    "        returns_matrix: シード別リターン行列 (shape: [seeds, nb_mesh])\n",
    "    \"\"\"\n",
    "    print(f\"  評価中: {policy_path}\")\n",
    "    \n",
    "    # 基底環境を作成\n",
    "    base_env_id = env_id_from_name(env_name)\n",
    "    base_env = gym.make(base_env_id)\n",
    "    \n",
    "    # エージェントを構築\n",
    "    if method in (\n",
    "        \"M2TD3\",\n",
    "        \"soft-actor-M2TD3\",\n",
    "        \"soft-omega-M2TD3\",\n",
    "        \"soft-omega-M2TD3-high\",\n",
    "        \"soft-omega-M2TD3-low\",\n",
    "        \"soft-omega-M2TD3-ex-low\",\n",
    "        \"only-soft-omega-M2TD3\",\n",
    "    ):\n",
    "        # soft-actor-M2TD3 と soft-omega-M2TD3系（high/low/ex-low含む）も M2TD3 と同じエージェント実装で評価\n",
    "        agent = build_agent_m2td3(policy_path, base_env, device, tcrmdp_src)\n",
    "    elif method in (\"M2SAC\", \"Ablation-SAC\", \"Ablation-DR-SAC\"):\n",
    "        # (M2SAC / Ablation-SAC / Ablation-DR-SAC)\n",
    "        # GaussianPolicyNetwork の state_dict を読む（policy-*.pth）\n",
    "        agent = build_agent_m2sac(policy_path, base_env, device, tcrmdp_src)\n",
    "    elif method in (\"Ablation-TD3\",):\n",
    "        # Ablation TD3 は actor の state_dict だけを保存しているため専用ローダを使う\n",
    "        agent = build_agent_td3_actor_only(policy_path, base_env, device, tcrmdp_src)\n",
    "    else:  # DR or RARL (both use TD3)\n",
    "        agent = build_agent_td3(policy_path, base_env, device, tcrmdp_src)\n",
    "    \n",
    "    # 各環境（ω）で評価\n",
    "    returns_per_env = []  # List[List[float]], len=nb_mesh\n",
    "    \n",
    "    for env_idx, env in enumerate(env_set):\n",
    "        env_returns = []\n",
    "        for seed in range(seeds):\n",
    "            ret = rollout_return(env, agent, seed=seed, max_steps=max_steps)\n",
    "            env_returns.append(ret)\n",
    "        returns_per_env.append(env_returns)\n",
    "    \n",
    "    # 平均と標準誤差を計算\n",
    "    mean_per_env = np.array([np.mean(rets) for rets in returns_per_env])\n",
    "    sem_per_env = np.array([\n",
    "        np.std(rets, ddof=1) / np.sqrt(len(rets)) if len(rets) > 1 else 0.0\n",
    "        for rets in returns_per_env\n",
    "    ])\n",
    "    \n",
    "    # シード別行列 (seeds, nb_mesh)\n",
    "    returns_matrix = np.array(returns_per_env).T\n",
    "    \n",
    "    return mean_per_env, sem_per_env, returns_matrix\n",
    "\n",
    "print(\"✓ 評価関数定義完了\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 5. 方策パスの解決と評価実行\n",
    "\n",
    "import pickle\n",
    "\n",
    "# ポリシー評価のキャッシュ（方策パスごと）\n",
    "_policy_cache_dir = workspace_root / \"cache\"\n",
    "_policy_cache_path = _policy_cache_dir / \"policy_eval_cache.pkl\"\n",
    "os.makedirs(_policy_cache_dir, exist_ok=True)\n",
    "\n",
    "try:\n",
    "    with open(_policy_cache_path, \"rb\") as f:\n",
    "        policy_eval_cache = pickle.load(f)\n",
    "    print(f\"✓ ポリシー評価キャッシュを読み込み: {_policy_cache_path} (entries={len(policy_eval_cache)})\")\n",
    "except FileNotFoundError:\n",
    "    policy_eval_cache = {}\n",
    "    print(f\"✓ ポリシー評価キャッシュ: 新規作成 ({_policy_cache_path})\")\n",
    "except Exception as e:\n",
    "    print(f\"⚠ ポリシー評価キャッシュの読み込みに失敗したため無視します: {e}\")\n",
    "    policy_eval_cache = {}\n",
    "\n",
    "\n",
    "def _policy_cache_key(method_name: str, policy_path: str) -> str:\n",
    "    \"\"\"キャッシュ用のキーを生成（環境設定＋手法＋絶対パス）\"\"\"\n",
    "    abs_path = os.path.abspath(policy_path)\n",
    "    return \"|\".join(\n",
    "        [\n",
    "            f\"env={env_name}\",\n",
    "            f\"dim={nb_dim}\",\n",
    "            f\"mesh={nb_mesh}\",\n",
    "            f\"seeds={seeds}\",\n",
    "            f\"max_steps={max_steps}\",\n",
    "            f\"method={method_name}\",\n",
    "            f\"path={abs_path}\",\n",
    "        ]\n",
    "    )\n",
    "\n",
    "\n",
    "def get_policy_eval(policy_path: str, method_name: str):\n",
    "    \"\"\"\n",
    "    方策評価をキャッシュ付きで取得するヘルパー。\n",
    "    既に計算済みならディスク上の結果を再利用し、未評価なら evaluate_policy を実行して保存する。\n",
    "    \"\"\"\n",
    "    key = _policy_cache_key(method_name, policy_path)\n",
    "    if key in policy_eval_cache:\n",
    "        cached = policy_eval_cache[key]\n",
    "        print(f\"  キャッシュ再利用: {policy_path}\")\n",
    "        return (\n",
    "            np.array(cached[\"mean_per_env\"]),\n",
    "            np.array(cached[\"sem_per_env\"]),\n",
    "            np.array(cached[\"returns_matrix\"]),\n",
    "        )\n",
    "\n",
    "    mean_per_env, sem_per_env, returns_matrix = evaluate_policy(\n",
    "        policy_path=policy_path,\n",
    "        method=method_name,\n",
    "        env_set=env_set,\n",
    "        omega_vals=omega_vals,\n",
    "        seeds=seeds,\n",
    "        max_steps=max_steps,\n",
    "        device=device,\n",
    "        tcrmdp_src=tcrmdp_src,\n",
    "    )\n",
    "    policy_eval_cache[key] = {\n",
    "        \"mean_per_env\": mean_per_env,\n",
    "        \"sem_per_env\": sem_per_env,\n",
    "        \"returns_matrix\": returns_matrix,\n",
    "    }\n",
    "    return mean_per_env, sem_per_env, returns_matrix\n",
    "\n",
    "\n",
    "def _extract_step_from_policy(path: str) -> int | None:\n",
    "    \"\"\"方策ファイル名からステップ数を抽出（例: policy-5000000.pth -> 5000000）\"\"\"\n",
    "    base = os.path.basename(path)\n",
    "    if base.startswith(\"policy-\") and base.endswith(\".pth\"):\n",
    "        middle = base[len(\"policy-\") : -len(\".pth\")]\n",
    "        if middle.isdigit():\n",
    "            return int(middle)\n",
    "    return None\n",
    "\n",
    "def resolve_policy_paths(globs: List[str]) -> List[str]:\n",
    "    \"\"\"globパターンから方策パスを解決（最新/最大ステップを選択）\"\"\"\n",
    "    resolved = []\n",
    "    for pattern in globs:\n",
    "        # workspace_rootを基準にした相対パスを処理\n",
    "        if not os.path.isabs(pattern):\n",
    "            pattern = str(workspace_root / pattern)\n",
    "        \n",
    "        expanded = os.path.expandvars(os.path.expanduser(pattern))\n",
    "        if any(ch in expanded for ch in [\"*\", \"?\", \"[\"]):\n",
    "            candidates = glob.glob(expanded, recursive=True)\n",
    "            if not candidates:\n",
    "                print(f\"  警告: パターンに一致するファイルなし: {expanded}\")\n",
    "                continue\n",
    "            \n",
    "            # agent.pthの場合はそのまま使用\n",
    "            agent_files = [p for p in candidates if os.path.basename(p) == \"agent.pth\"]\n",
    "            if agent_files:\n",
    "                resolved.extend(agent_files)\n",
    "                continue\n",
    "            \n",
    "            # policy-*.pthの場合は最大ステップを選択\n",
    "            policy_files = [p for p in candidates if _extract_step_from_policy(p) is not None]\n",
    "            if policy_files:\n",
    "                # 各ディレクトリごとに最新のpolicyファイルを選択\n",
    "                policy_by_dir = {}\n",
    "                for p in policy_files:\n",
    "                    dir_path = os.path.dirname(p)\n",
    "                    step = _extract_step_from_policy(p)\n",
    "                    if dir_path not in policy_by_dir:\n",
    "                        policy_by_dir[dir_path] = p\n",
    "                    else:\n",
    "                        existing_step = _extract_step_from_policy(policy_by_dir[dir_path])\n",
    "                        if step and existing_step and step > existing_step:\n",
    "                            policy_by_dir[dir_path] = p\n",
    "                resolved.extend(policy_by_dir.values())\n",
    "            else:\n",
    "                # その他の場合は最新のものを選択\n",
    "                resolved.append(max(candidates, key=lambda p: os.path.getmtime(p)))\n",
    "        else:\n",
    "            if os.path.exists(expanded):\n",
    "                resolved.append(expanded)\n",
    "            else:\n",
    "                print(f\"  警告: ファイルが見つかりません: {expanded}\")\n",
    "    return resolved\n",
    "\n",
    "# 各手法の評価結果を格納\n",
    "method_results: Dict[str, Dict[str, np.ndarray]] = {}\n",
    "\n",
    "for method_name, globs in method_to_globs.items():\n",
    "    if not globs:\n",
    "        print(f\"{method_name}: 方策パスが指定されていません。スキップします。\")\n",
    "        continue\n",
    "    \n",
    "    print(f\"\\n{method_name} の評価を開始...\")\n",
    "    policy_paths = resolve_policy_paths(globs)\n",
    "    \n",
    "    if not policy_paths:\n",
    "        print(f\"  {method_name}: 有効な方策パスが見つかりませんでした。\")\n",
    "        continue\n",
    "    \n",
    "    print(f\"  見つかった方策数: {len(policy_paths)}\")\n",
    "    for pp in policy_paths:\n",
    "        print(f\"    - {pp}\")\n",
    "    \n",
    "    # 各方策を評価（キャッシュ付き）\n",
    "    policy_means = []\n",
    "    policy_sems = []\n",
    "    policy_returns_list = []  # 各方策のシード別行列を保持 (shape: [seeds, nb_mesh])\n",
    "    \n",
    "    for policy_path in policy_paths:\n",
    "        try:\n",
    "            mean_per_env, sem_per_env, returns_matrix = get_policy_eval(\n",
    "                policy_path=policy_path,\n",
    "                method_name=method_name,\n",
    "            )\n",
    "            policy_means.append(mean_per_env)\n",
    "            policy_sems.append(sem_per_env)\n",
    "            policy_returns_list.append(returns_matrix)\n",
    "        except Exception as e:\n",
    "            print(f\"  エラー: {policy_path} の評価に失敗: {e}\")\n",
    "            import traceback\n",
    "            traceback.print_exc()\n",
    "            continue\n",
    "    \n",
    "    if not policy_means:\n",
    "        print(f\"  {method_name}: 評価に成功した方策がありません。\")\n",
    "        continue\n",
    "    \n",
    "    # 手法全体の平均と標準誤差を計算\n",
    "    policy_means = np.array(policy_means)  # shape: (num_policies, nb_mesh)\n",
    "    policy_sems = np.array(policy_sems)\n",
    "    \n",
    "    # 手法平均\n",
    "    method_mean = np.mean(policy_means, axis=0)\n",
    "    \n",
    "    # 標準誤差の計算\n",
    "    if len(policy_means) > 1:\n",
    "        # 複数方策がある場合: 方策間の標準誤差\n",
    "        method_sem = np.std(policy_means, axis=0, ddof=1) / np.sqrt(len(policy_means))\n",
    "    else:\n",
    "        # 単一方策の場合: エピソード間の標準誤差\n",
    "        method_sem = policy_sems[0]\n",
    "    \n",
    "    # 最悪平均（min over ω）\n",
    "    worst_avg = float(np.min(method_mean))\n",
    "    \n",
    "    # 全シード分のリターン行列を結合 (num_total_seeds, nb_mesh)\n",
    "    all_returns = np.concatenate(policy_returns_list, axis=0) if policy_returns_list else None\n",
    "    \n",
    "    method_results[method_name] = {\n",
    "        \"mean\": method_mean,\n",
    "        \"sem\": method_sem,\n",
    "        \"worst_avg\": worst_avg,\n",
    "        \"all_returns\": all_returns,\n",
    "        \"policy_means\": np.array(policy_means),  # shape: (num_policies, nb_mesh)\n",
    "        \"policy_sems\": np.array(policy_sems),    # shape: (num_policies, nb_mesh)\n",
    "        \"policy_paths\": policy_paths,\n",
    "    }\n",
    "    \n",
    "    print(f\"  {method_name} 完了: 最悪平均 = {worst_avg:.2f}\")\n",
    "\n",
    "# 評価結果のキャッシュを保存\n",
    "try:\n",
    "    with open(_policy_cache_path, \"wb\") as f:\n",
    "        pickle.dump(policy_eval_cache, f)\n",
    "    print(f\"\\n✓ ポリシー評価キャッシュを保存: {_policy_cache_path} (entries={len(policy_eval_cache)})\")\n",
    "except Exception as e:\n",
    "    print(f\"\\n⚠ ポリシー評価キャッシュの保存に失敗しました: {e}\")\n",
    "\n",
    "print(f\"\\n✓ 全評価完了。評価された手法数: {len(method_results)}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 6. 可視化（平均・標準誤差・最悪平均）\n",
    "# 縦軸範囲 (ymin, ymax) もインタラクティブに指定可能\n",
    "\n",
    "import ipywidgets as widgets\n",
    "from IPython.display import display\n",
    "\n",
    "available_methods = list(method_results.keys())\n",
    "method_checkboxes = [\n",
    "    widgets.Checkbox(value=True, description=m, indent=False) for m in available_methods\n",
    "]\n",
    "\n",
    "# デフォルトの縦軸下限・上限を自動決定\n",
    "def get_default_ylim():\n",
    "    # 全手法の全meanとsemから範囲を計算\n",
    "    means = []\n",
    "    sems = []\n",
    "    for r in method_results.values():\n",
    "        means.append(r[\"mean\"])\n",
    "        sems.append(r[\"sem\"])\n",
    "    if means:\n",
    "        means = np.stack(means)\n",
    "        sems = np.stack(sems)\n",
    "        global_min = float(np.min(means - sems))\n",
    "        global_max = float(np.max(means + sems))\n",
    "        ymin = max(0, global_min - 0.05 * abs(global_max - global_min))\n",
    "        ymax = global_max + 0.05 * abs(global_max - global_min)\n",
    "        return ymin, ymax\n",
    "    return 0.0, 1.0\n",
    "\n",
    "default_ymin, default_ymax = get_default_ylim()\n",
    "ymin_widget = widgets.FloatText(value=default_ymin, description=\"ymin\", step=1.0)\n",
    "ymax_widget = widgets.FloatText(value=default_ymax, description=\"ymax\", step=1.0)\n",
    "\n",
    "def get_selected_methods():\n",
    "    return [cb.description for cb in method_checkboxes if cb.value]\n",
    "\n",
    "def plot_results(selected_methods, ymin=None, ymax=None):\n",
    "    plot_targets = list(selected_methods) if selected_methods else available_methods\n",
    "    missing_methods = [m for m in plot_targets if m not in method_results]\n",
    "    if missing_methods:\n",
    "        print(f\"警告: 評価結果に存在しない手法をスキップします: {missing_methods}\")\n",
    "    plot_targets = [m for m in plot_targets if m in method_results]\n",
    "    if not plot_targets:\n",
    "        raise ValueError(\"可視化対象の手法がありません。手法を1つ以上選択してください。\")\n",
    "\n",
    "    # 色の設定（表示したい手法だけ色を定義。コメントアウトで非表示）\n",
    "    colors = {\n",
    "        \"DR\": \"red\",\n",
    "        \"soft-actor-M2TD3\": \"blue\",\n",
    "        \"soft-omega-M2TD3\": \"cyan\",\n",
    "        \"soft-omega-M2TD3-high\": \"teal\",\n",
    "        \"soft-omega-M2TD3-low\": \"purple\",\n",
    "        \"soft-omega-M2TD3-ex-low\": \"slateblue\",\n",
    "        \"only-soft-omega-M2TD3\": \"deepskyblue\",\n",
    "        \"M2SAC\": \"magenta\",\n",
    "        \"M2TD3\": \"green\",\n",
    "        \"RARL\": \"pink\",\n",
    "\n",
    "        # Ablation baselines\n",
    "        \"Ablation-SAC\": \"orange\",\n",
    "        \"Ablation-DR-SAC\": \"gray\",\n",
    "        \"Ablation-TD3\": \"brown\",\n",
    "    }\n",
    "    label_map = {\n",
    "        \"soft-actor-M2TD3\": \"M2TD3 + SW(PI)\",\n",
    "        \"soft-omega-M2TD3\": \"M2TD3 + SW(PI, EI)\",\n",
    "        \"only-soft-omega-M2TD3\": \"M2TD3 + SW(EI)\",\n",
    "        \"soft-omega-M2TD3-high\": \"α=4000\",\n",
    "        \"soft-omega-M2TD3-low\": \"α=20\",\n",
    "        \"soft-omega-M2TD3-ex-low\": \"α=1\",\n",
    "        \"M2SAC\": \"M2SAC\",\n",
    "        \"DR\": \"DR(TD3)\",\n",
    "        \"RARL\": \"RARL(TD3)\",\n",
    "\n",
    "        # Ablation baselines\n",
    "        \"Ablation-SAC\": \"SAC\",\n",
    "        \"Ablation-DR-SAC\": \"DR(SAC)\",\n",
    "        \"Ablation-TD3\": \"TD3\",\n",
    "    }\n",
    "\n",
    "    fig, ax = plt.subplots(figsize=(10, 6))\n",
    "\n",
    "    for method_name in plot_targets:\n",
    "        results = method_results[method_name]\n",
    "        mean = results[\"mean\"]\n",
    "        sem = results[\"sem\"]\n",
    "        worst_avg = results[\"worst_avg\"]\n",
    "        color = colors.get(method_name)\n",
    "        if color is None:\n",
    "            print(f\"スキップ: colors で色を設定していない手法: {method_name}\")\n",
    "            continue\n",
    "\n",
    "        legend_label = label_map.get(method_name, method_name)\n",
    "        ax.plot(omega_vals, mean, label=legend_label, color=color, linewidth=2)\n",
    "\n",
    "        ax.fill_between(\n",
    "            omega_vals,\n",
    "            mean - sem,\n",
    "            mean + sem,\n",
    "            alpha=0.3,\n",
    "            color=color,\n",
    "        )\n",
    "        ax.axhline(\n",
    "            y=worst_avg,\n",
    "            color=color,\n",
    "            linestyle=\"--\",\n",
    "            linewidth=1.5,\n",
    "            alpha=0.7,\n",
    "        )\n",
    "\n",
    "    ax.set_xlabel(\"Uncertainty parameter\", fontsize=16)\n",
    "    ax.set_ylabel(\"Cumulative reward\", fontsize=16)\n",
    "    ax.legend(fontsize=14)\n",
    "    ax.tick_params(axis=\"both\", labelsize=12)\n",
    "    ax.grid(True, alpha=0.3)\n",
    "\n",
    "    # 縦軸範囲のインタラクティブ制御\n",
    "    ymin_ = ymin if ymin is not None else 0\n",
    "    ymax_ = ymax if ymax is not None else ax.get_ylim()[1]\n",
    "    if ymax_ <= ymin_:\n",
    "        ymax_ = ymin_ + 1.0\n",
    "    ax.set_ylim(bottom=ymin_, top=ymax_)\n",
    "\n",
    "    plt.tight_layout()\n",
    "\n",
    "    os.makedirs(workspace_root / \"figs\", exist_ok=True)\n",
    "    output_path = workspace_root / \"figs\" / f\"omega_sweep_{env_name}_{nb_dim}D.png\"\n",
    "    plt.savefig(output_path, dpi=300, bbox_inches=\"tight\")\n",
    "    print(f\"✓ 図を保存: {output_path}\")\n",
    "\n",
    "    plt.show()\n",
    "\n",
    "    # --- グラフ下にサマリ表を表示（全点平均 ± 標準誤差、最悪平均=破線） ---\n",
    "    summary_rows = []\n",
    "    for method_name in plot_targets:\n",
    "        results = method_results[method_name]\n",
    "        mean = np.asarray(results[\"mean\"], dtype=float)\n",
    "        sem = np.asarray(results[\"sem\"], dtype=float)\n",
    "\n",
    "        worst_avg = float(results.get(\"worst_avg\", np.min(mean)))\n",
    "        avg_all_points = float(np.mean(mean))\n",
    "        avg_sem_all_points = float(np.mean(sem))\n",
    "\n",
    "        summary_rows.append(\n",
    "            {\n",
    "                \"手法\": label_map.get(method_name, method_name),\n",
    "                \"全点平均 ± 標準誤差\": f\"{avg_all_points:.2f} ± {avg_sem_all_points:.2f}\",\n",
    "                \"最悪平均(破線)\": worst_avg,\n",
    "            }\n",
    "        )\n",
    "\n",
    "    try:\n",
    "        import pandas as pd\n",
    "\n",
    "        df = pd.DataFrame(summary_rows)\n",
    "        # \"最悪平均(破線)\"列は表示用に丸める\n",
    "        df[\"最悪平均(破線)\"] = df[\"最悪平均(破線)\"].map(lambda x: float(np.round(x, 2)))\n",
    "        display(df)\n",
    "    except Exception:\n",
    "        print(\"\\n--- 集計（全点平均 ± 標準誤差 / 最悪平均=破線）---\")\n",
    "        for r in summary_rows:\n",
    "            print(\n",
    "                f\"{r['手法']}: 全点平均 ± 標準誤差={r['全点平均 ± 標準誤差']}, \"\n",
    "                f\"最悪平均(破線)={r['最悪平均(破線)']:.2f}\"\n",
    "            )\n",
    "\n",
    "def update_plot(*args):\n",
    "    with output_area:\n",
    "        output_area.clear_output()\n",
    "        try:\n",
    "            plot_results(get_selected_methods(), ymin=ymin_widget.value, ymax=ymax_widget.value)\n",
    "        except Exception as e:\n",
    "            print(f\"プロット失敗: {e}\")\n",
    "\n",
    "output_area = widgets.Output()\n",
    "\n",
    "for cb in method_checkboxes:\n",
    "    cb.observe(lambda change: update_plot() if change[\"name\"] == \"value\" else None, names=\"value\")\n",
    "ymin_widget.observe(lambda change: update_plot() if change[\"name\"] == \"value\" else None, names=\"value\")\n",
    "ymax_widget.observe(lambda change: update_plot() if change[\"name\"] == \"value\" else None, names=\"value\")\n",
    "\n",
    "# 初期描画\n",
    "with output_area:\n",
    "    plot_results(get_selected_methods(), ymin=ymin_widget.value, ymax=ymax_widget.value)\n",
    "\n",
    "controls = widgets.VBox([\n",
    "    widgets.HBox([widgets.Label(\"<b>手法</b>\", layout=widgets.Layout(width=\"70px\")), ymin_widget, ymax_widget]),\n",
    "    *method_checkboxes\n",
    "])\n",
    "\n",
    "display(widgets.VBox([controls, output_area]))\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 既存の手法カラー（凡例の補助に使用）\n",
    "colors = {\n",
    "    \"M2TD3\": \"green\",\n",
    "    \"soft-actor-M2TD3\": \"blue\",  # 別枠だが M2TD3 系として扱う\n",
    "    \"soft-omega-M2TD3\": \"cyan\",  # M2TD3 系として扱う\n",
    "    \"soft-omega-M2TD3-high\": \"teal\",  # soft-omega-M2TD3-high用の色\n",
    "    \"soft-omega-M2TD3-low\": \"darkcyan\",  # soft-omega-M2TD3-low用の色\n",
    "    \"soft-omega-M2TD3-ex-low\": \"slateblue\",  # soft-omega-M2TD3-ex-low用の色\n",
    "    \"M2SAC\": \"orange\",           # M2SAC 用の色\n",
    "    \"DR\": \"red\",\n",
    "    \"RARL\": \"purple\",\n",
    "}\n",
    "\n",
    "# 対象手法は結果があるものに限定\n",
    "target_methods = [m for m in [\"M2TD3\", \"soft-actor-M2TD3\", \"soft-omega-M2TD3\", \"soft-omega-M2TD3-high\", \"soft-omega-M2TD3-low\", \"soft-omega-M2TD3-ex-low\", \"M2SAC\", \"DR\", \"RARL\"] if m in method_results]\n",
    "policy_cmap = plt.get_cmap('tab10')\n",
    "\n",
    "\n",
    "def ensure_policy_means(method_name: str, results: dict) -> Optional[np.ndarray]:\n",
    "    \"\"\"method_results内にpolicy_meansが無ければ再計算して補完\"\"\"\n",
    "    policy_means = results.get(\"policy_means\")\n",
    "    if policy_means is not None:\n",
    "        return policy_means\n",
    "\n",
    "    policy_paths = results.get(\"policy_paths\", [])\n",
    "    if not policy_paths:\n",
    "        print(f\"{method_name}: policy_meansが見つからず、policy_pathsもありません\")\n",
    "        return None\n",
    "\n",
    "    print(f\"{method_name}: policy_meansが見つからないため再計算します...\")\n",
    "    recomputed_means = []\n",
    "    for p in policy_paths:\n",
    "        try:\n",
    "            mean_per_env, _, _ = get_policy_eval(\n",
    "                policy_path=p,\n",
    "                method_name=method_name,\n",
    "            )\n",
    "            recomputed_means.append(mean_per_env)\n",
    "        except Exception as e:\n",
    "            print(f\"  再計算失敗: {p}: {e}\")\n",
    "\n",
    "    if not recomputed_means:\n",
    "        print(f\"{method_name}: policy_meansの再計算に失敗しました\")\n",
    "        return None\n",
    "\n",
    "    policy_means = np.array(recomputed_means)\n",
    "    results[\"policy_means\"] = policy_means  # 次回以降のために保存\n",
    "    return policy_means\n",
    "\n",
    "\n",
    "# 手法ごとに、学習シード（=方策パス）ごとの平均曲線を別々の図（台紙）に重ねてプロットする\n",
    "for method_name in target_methods:\n",
    "    results = method_results.get(method_name)\n",
    "    if results is None:\n",
    "        continue\n",
    "\n",
    "    policy_means = ensure_policy_means(method_name, results)\n",
    "    if policy_means is None:\n",
    "        continue\n",
    "\n",
    "    fig, ax = plt.subplots(figsize=(10, 6))\n",
    "\n",
    "    # 同じ手法の中では、シードごとに色を変えて可視化\n",
    "    for idx, mean_curve in enumerate(policy_means):\n",
    "        seed_color = policy_cmap(idx % 10)  # シード番号ごとに色を変える\n",
    "        ax.plot(omega_vals, mean_curve, label=f\"Seed {idx+1}\", alpha=0.8, color=seed_color)\n",
    "    ax.set_xlabel(\"Uncertainty parameter\", fontsize=16)\n",
    "    ax.set_ylabel(\"Return\", fontsize=16)\n",
    "    ax.set_title(f\"Policy Seeds: {method_name} ({env_name}, {nb_dim}D)\", fontsize=11)\n",
    "    ax.legend(fontsize=12)\n",
    "    ax.tick_params(axis=\"both\", labelsize=12)\n",
    "    ax.grid(True, alpha=0.3)\n",
    "    plt.tight_layout()\n",
    "\n",
    "    # 台紙ごとに保存\n",
    "    plot_path = workspace_root / \"figs\" / f\"policy_seeds_{method_name}_{env_name}_{nb_dim}D.png\"\n",
    "    plt.savefig(plot_path, dpi=300, bbox_inches=\"tight\")\n",
    "    print(f\"✓ {method_name} の図を保存: {plot_path}\")\n",
    "    plt.show()\n",
    "\n",
    "# 全手法×全方策（学習シード）を1枚の図に集約\n",
    "fig, ax = plt.subplots(figsize=(10, 6))\n",
    "\n",
    "# 各手法ごとに、すべてのSeedの曲線を同じ色・同じ濃さで描画\n",
    "for i, method_name in enumerate(target_methods):\n",
    "    results = method_results.get(method_name)\n",
    "    if results is None:\n",
    "        continue\n",
    "\n",
    "    policy_means = ensure_policy_means(method_name, results)\n",
    "    if policy_means is None:\n",
    "        continue\n",
    "\n",
    "    policy_paths = results.get(\"policy_paths\", [])\n",
    "    color = colors.get(method_name, policy_cmap(i % 10))  # 手法ごとの色を固定\n",
    "    for idx, mean_curve in enumerate(policy_means):\n",
    "        label_suffix = os.path.basename(policy_paths[idx]) if idx < len(policy_paths) else f\"policy{idx}\"\n",
    "        # 最初のseedだけlabelを書く（それ以降は重複しないようNoneに）\n",
    "        label = method_name if idx == 0 else None\n",
    "        ax.plot(\n",
    "            omega_vals,\n",
    "            mean_curve,\n",
    "            color=color,\n",
    "            linewidth=2,\n",
    "            alpha=0.7,\n",
    "            label=label\n",
    "        )\n",
    "\n",
    "ax.set_xlabel(\"Uncertainty parameter\", fontsize=16)\n",
    "ax.set_ylabel(\"Return\", fontsize=16)\n",
    "ax.set_title(f\"Policy Evaluation by Seed: {env_name} ({nb_dim}D)\", fontsize=11)\n",
    "ax.tick_params(axis=\"both\", labelsize=12)\n",
    "# ラベルは手法ごとのみにする\n",
    "handles, labels = ax.get_legend_handles_labels()\n",
    "from collections import OrderedDict\n",
    "by_label = OrderedDict()\n",
    "for h, l in zip(handles, labels):\n",
    "    if l is not None and l not in by_label:\n",
    "        by_label[l] = h\n",
    "ax.legend(by_label.values(), by_label.keys(), fontsize=14)\n",
    "ax.grid(True, alpha=0.3)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 7. オプション: 結果のキャッシュ保存・読み込み\n",
    "\n",
    "if cache_path:\n",
    "    cache_file = workspace_root / cache_path\n",
    "    os.makedirs(cache_file.parent, exist_ok=True)\n",
    "    \n",
    "    # 保存\n",
    "    cache_data = {\n",
    "        \"omega_vals\": omega_vals,\n",
    "        \"method_results\": {k: {kk: vv.tolist() if isinstance(vv, np.ndarray) else vv \n",
    "                               for kk, vv in v.items()} \n",
    "                           for k, v in method_results.items()},\n",
    "        \"config\": {\n",
    "            \"env_name\": env_name,\n",
    "            \"nb_dim\": nb_dim,\n",
    "            \"nb_mesh\": nb_mesh,\n",
    "            \"seeds\": seeds,\n",
    "            \"max_steps\": max_steps,\n",
    "        }\n",
    "    }\n",
    "    np.savez_compressed(cache_file, **cache_data)\n",
    "    print(f\"✓ キャッシュを保存: {cache_file}\")\n",
    "    \n",
    "    # 読み込み例（コメントアウト）\n",
    "    # loaded = np.load(cache_file, allow_pickle=True)\n",
    "    # omega_vals = loaded[\"omega_vals\"]\n",
    "    # method_results = {k: {kk: np.array(vv) if isinstance(vv, list) else vv \n",
    "    #                       for kk, vv in v.item().items()} \n",
    "    #                  for k, v in loaded[\"method_results\"].item().items()}\n",
    "else:\n",
    "    print(\"キャッシュは無効化されています\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 8. 任意パラメタ点での方策動画生成\n",
    "import imageio.v2 as imageio\n",
    "from datetime import datetime\n",
    "\n",
    "# --- 描画で床が途切れる対策（学習環境の物理は変えない） ----------------------\n",
    "# MuJoCo の plane は「接触判定としては（多くの場合）無限」ですが、\n",
    "# “見た目”は geom_size / zfar 等で有限に見えることがあります。\n",
    "# 動画生成時だけ、床の描画範囲（plane geom の size）と zfar を大きくして\n",
    "# 「床が途中で消えて空中に飛び出したように見える」を防ぎます。\n",
    "try:\n",
    "    import mujoco  # mujoco>=2\n",
    "except Exception:\n",
    "    mujoco = None\n",
    "\n",
    "\n",
    "def _widen_mujoco_visual_floor(env: gym.Env, half_length: float = 2000.0, half_width: float = 200.0, zfar_margin: float = 4.0) -> None:\n",
    "    \"\"\"床の描画範囲と zfar だけを伸ばす（stat.extent は触らない）\"\"\"\n",
    "    if mujoco is None:\n",
    "        return\n",
    "    try:\n",
    "        model = env.unwrapped.model\n",
    "    except Exception:\n",
    "        return\n",
    "\n",
    "    try:\n",
    "        ngeom = int(getattr(model, \"ngeom\", 0))\n",
    "    except Exception:\n",
    "        ngeom = 0\n",
    "\n",
    "    for gid in range(ngeom):\n",
    "        try:\n",
    "            gtype = int(model.geom_type[gid])\n",
    "        except Exception:\n",
    "            continue\n",
    "\n",
    "        # mjGEOM_PLANE の描画サイズを拡張\n",
    "        try:\n",
    "            if gtype == int(mujoco.mjtGeom.mjGEOM_PLANE):\n",
    "                size = model.geom_size[gid].copy()\n",
    "                if size.shape[0] >= 2:\n",
    "                    size[0] = max(float(size[0]), float(half_length))\n",
    "                    size[1] = max(float(size[1]), float(half_width))\n",
    "                    model.geom_size[gid] = size\n",
    "                continue\n",
    "        except Exception:\n",
    "            pass\n",
    "\n",
    "        # 2) 名前に floor/ground 等が含まれる床っぽい geom も拡張\n",
    "        try:\n",
    "            name = mujoco.mj_id2name(model, mujoco.mjtObj.mjOBJ_GEOM, gid)\n",
    "        except Exception:\n",
    "            name = None\n",
    "        if not name:\n",
    "            continue\n",
    "        low = str(name).lower()\n",
    "        if not any(k in low for k in (\"floor\", \"ground\", \"terrain\", \"track\")):\n",
    "            continue\n",
    "        try:\n",
    "            size = model.geom_size[gid].copy()\n",
    "            if size.shape[0] >= 2:\n",
    "                size[0] = max(float(size[0]), float(half_length))\n",
    "                size[1] = max(float(size[1]), float(half_width))\n",
    "                model.geom_size[gid] = size\n",
    "        except Exception:\n",
    "            continue\n",
    "\n",
    "    # 遠方クリップを延ばす（extent には触れない）\n",
    "    try:\n",
    "        model.vis.map.zfar = max(float(model.vis.map.zfar), float(half_length) * float(zfar_margin))\n",
    "    except Exception:\n",
    "        pass\n",
    "\n",
    "\n",
    "def _set_tracking_camera(env: gym.Env, distance: float = 6.0, elevation: float = -10.0, azimuth: float = 90.0) -> None:\n",
    "    \"\"\"カメラをトラッキングにして画面外に逃げないようにする（描画専用）\"\"\"\n",
    "    if not hasattr(env, \"mujoco_renderer\"):\n",
    "        return\n",
    "\n",
    "    bodyid = None\n",
    "    try:\n",
    "        model = env.unwrapped.model\n",
    "        if hasattr(model, \"body_name2id\"):\n",
    "            for name in [\"torso\", \"root\", \"pelvis\", \"torso1\"]:\n",
    "                try:\n",
    "                    bodyid = int(model.body_name2id(name))\n",
    "                    break\n",
    "                except Exception:\n",
    "                    continue\n",
    "    except Exception:\n",
    "        pass\n",
    "\n",
    "    cam_settings = {\n",
    "        \"distance\": float(distance),\n",
    "        \"elevation\": float(elevation),\n",
    "        \"azimuth\": float(azimuth),\n",
    "    }\n",
    "    if bodyid is not None:\n",
    "        cam_settings[\"trackbodyid\"] = bodyid\n",
    "\n",
    "    # 新しめの gymnasium.mujoco_renderer では set_camera_settings が使える\n",
    "    try:\n",
    "        env.mujoco_renderer.set_camera_settings(cam_settings)\n",
    "    except Exception:\n",
    "        pass\n",
    "\n",
    "    # 旧来の viewer.cam API も併用しておく\n",
    "    try:\n",
    "        cam = getattr(env.mujoco_renderer, \"cam\", None)\n",
    "        if cam is not None:\n",
    "            if bodyid is not None and hasattr(cam, \"trackbodyid\"):\n",
    "                cam.trackbodyid = bodyid\n",
    "            if hasattr(cam, \"distance\"):\n",
    "                cam.distance = max(float(cam.distance), float(distance))\n",
    "            if hasattr(cam, \"elevation\"):\n",
    "                cam.elevation = float(elevation)\n",
    "            if hasattr(cam, \"azimuth\"):\n",
    "                cam.azimuth = float(azimuth)\n",
    "    except Exception:\n",
    "        pass\n",
    "\n",
    "\n",
    "# --- 設定ここから -----------------------------------------------------------\n",
    "default_policy = None\n",
    "# soft-actor-M2TD3 があればそれを優先し、なければ soft-omega-M2TD3系, M2SAC, Ablation, M2TD3, DR, RARL の順に探す\n",
    "for m in [\n",
    "    \"soft-actor-M2TD3\",\n",
    "    \"soft-omega-M2TD3\",\n",
    "    \"soft-omega-M2TD3-high\",\n",
    "    \"soft-omega-M2TD3-low\",\n",
    "    \"soft-omega-M2TD3-ex-low\",\n",
    "    \"M2SAC\",\n",
    "    \"Ablation-SAC\",\n",
    "    \"Ablation-DR-SAC\",\n",
    "    \"Ablation-TD3\",\n",
    "    \"M2TD3\",\n",
    "    \"DR\",\n",
    "    \"RARL\",\n",
    "]:\n",
    "    cand = method_results.get(m, {}).get(\"policy_paths\")\n",
    "    if cand:\n",
    "        default_policy = cand[0]\n",
    "        default_method = m\n",
    "        break\n",
    "\n",
    "video_policy_path = default_policy  # 学習済み方策のパスを指定\n",
    "video_method = default_method if 'default_method' in locals() else \"M2TD3\"  # \"M2TD3\", \"M2SAC\", \"Ablation-SAC\", \"Ablation-DR-SAC\", \"Ablation-TD3\", \"soft-actor-M2TD3\", \"soft-omega-M2TD3\", \"soft-omega-M2TD3-high\", \"soft-omega-M2TD3-low\", \"soft-omega-M2TD3-ex-low\", \"DR\", \"RARL\"\n",
    "custom_omega_points = [omega_vals[0]]  # 例: [1.0, 15.0] または [{\"length\": 1.0}]\n",
    "video_episode_seeds = list(range(min(3, seeds)))  # seeds変数と揃えたい場合は list(range(seeds))\n",
    "video_max_steps = max_steps\n",
    "video_fps = 30\n",
    "video_output_dir = workspace_root / \"figs\" / \"videos\"\n",
    "os.makedirs(video_output_dir, exist_ok=True)\n",
    "\n",
    "# 描画用の床拡張（必要なら増やす）\n",
    "video_floor_half_length = 2000.0\n",
    "video_floor_half_width = 200.0\n",
    "# --- 設定ここまで -----------------------------------------------------------\n",
    "\n",
    "if not video_policy_path or not os.path.exists(video_policy_path):\n",
    "    raise FileNotFoundError(\"video_policy_path を存在する方策ファイルに設定してください\")\n",
    "\n",
    "base_env_id = env_id_from_name(env_name)\n",
    "base_env = gym.make(base_env_id)\n",
    "try:\n",
    "    if video_method in (\n",
    "        \"M2TD3\",\n",
    "        \"soft-actor-M2TD3\",\n",
    "        \"soft-omega-M2TD3\",\n",
    "        \"soft-omega-M2TD3-high\",\n",
    "        \"soft-omega-M2TD3-low\",\n",
    "        \"soft-omega-M2TD3-ex-low\",\n",
    "    ):\n",
    "        # soft-actor-M2TD3 と soft-omega-M2TD3系（high/low/ex-low含む）も M2TD3 と同じラッパーで読み込む\n",
    "        video_agent = build_agent_m2td3(video_policy_path, base_env, device, tcrmdp_src)\n",
    "    elif video_method in (\"M2SAC\", \"Ablation-SAC\", \"Ablation-DR-SAC\"):\n",
    "        # (M2SAC / Ablation-SAC / Ablation-DR-SAC)\n",
    "        video_agent = build_agent_m2sac(video_policy_path, base_env, device, tcrmdp_src)\n",
    "    elif video_method in (\"Ablation-TD3\",):\n",
    "        video_agent = build_agent_td3_actor_only(video_policy_path, base_env, device, tcrmdp_src)\n",
    "    else:\n",
    "        video_agent = build_agent_td3(video_policy_path, base_env, device, tcrmdp_src)\n",
    "finally:\n",
    "    base_env.close()\n",
    "\n",
    "param_keys = list(param_bounds.keys())\n",
    "\n",
    "\n",
    "def normalize_param_point(point) -> Dict[str, float]:\n",
    "    \"\"\"スカラー/リスト/辞書で指定したパラメタ点をdict化\"\"\"\n",
    "    if isinstance(point, dict):\n",
    "        param_dict = {k: float(point.get(k, param_bounds[k][0])) for k in param_keys}\n",
    "    else:\n",
    "        arr = np.atleast_1d(point).astype(float)\n",
    "        if arr.size != len(param_keys):\n",
    "            raise ValueError(f\"param数が一致しません: 期待={len(param_keys)} 実際={arr.size}\")\n",
    "        param_dict = {k: float(v) for k, v in zip(param_keys, arr)}\n",
    "    # 境界チェック\n",
    "    for k, v in param_dict.items():\n",
    "        lo, hi = map(float, param_bounds[k])\n",
    "        if not (lo <= v <= hi):\n",
    "            print(f\"⚠ {k}={v} が境界 [{lo}, {hi}] を超えています\")\n",
    "    return param_dict\n",
    "\n",
    "\n",
    "def instantiate_render_env(param_dict: Dict[str, float]) -> gym.Env:\n",
    "    \"\"\"rgb_arrayレンダリングを試みながら環境を生成\"\"\"\n",
    "    try:\n",
    "        env = ModifiedEnv(**param_dict, render_mode=\"rgb_array\")\n",
    "    except TypeError:\n",
    "        env = ModifiedEnv(**param_dict)\n",
    "\n",
    "    # 描画の床・遠方クリップを拡張 + カメラ追従（学習時の物理は変えない）\n",
    "    _widen_mujoco_visual_floor(env, half_length=video_floor_half_length, half_width=video_floor_half_width)\n",
    "    _set_tracking_camera(env)\n",
    "\n",
    "    # デバッグ: 先頭1回だけカメラ設定と geom サイズを表示して原因を追う\n",
    "    global _video_debug_done  # type: ignore\n",
    "    if \"_video_debug_done\" not in globals():\n",
    "        _video_debug_done = True\n",
    "        try:\n",
    "            model = env.unwrapped.model\n",
    "            print(\"[debug] geom_size[0:3]\", model.geom_size[:3])\n",
    "            print(\"[debug] vis.map.zfar\", getattr(model.vis.map, \"zfar\", None))\n",
    "            try:\n",
    "                viewer_cam = getattr(env.mujoco_renderer, \"cam\", None)\n",
    "                if viewer_cam:\n",
    "                    print(\"[debug] cam trackbodyid\", getattr(viewer_cam, \"trackbodyid\", None))\n",
    "                    print(\"[debug] cam distance/elevation/azimuth\", getattr(viewer_cam, \"distance\", None), getattr(viewer_cam, \"elevation\", None), getattr(viewer_cam, \"azimuth\", None))\n",
    "            except Exception:\n",
    "                pass\n",
    "        except Exception:\n",
    "            pass\n",
    "    return env\n",
    "\n",
    "\n",
    "def grab_frame(env: gym.Env):\n",
    "    frame = None\n",
    "    try:\n",
    "        frame = env.render()\n",
    "    except Exception:\n",
    "        frame = None\n",
    "    if frame is None and hasattr(env, \"mujoco_renderer\"):\n",
    "        try:\n",
    "            frame = env.mujoco_renderer.render(\"rgb_array\")\n",
    "        except Exception:\n",
    "            frame = None\n",
    "    return frame\n",
    "\n",
    "\n",
    "def record_episode_video(env: gym.Env, agent, seed: int, max_steps: int, fps: int, output_path: Path, reset_options: Dict[str, float]):\n",
    "    frames = []\n",
    "\n",
    "    # 学習時と同様に reset(options=omega_dict) 経路でパラメータを注入する\n",
    "    # （ModifiedEnv が init 引数で受け取っていても、ここで上書きされる実装が多い）\n",
    "    try:\n",
    "        obs, _ = env.reset(seed=seed, options=reset_options)\n",
    "    except TypeError:\n",
    "        obs, _ = env.reset(seed=seed)\n",
    "\n",
    "    first = grab_frame(env)\n",
    "    if first is not None:\n",
    "        frames.append(first)\n",
    "    done = truncated = False\n",
    "    steps = 0\n",
    "    while not (done or truncated) and steps < max_steps:\n",
    "        action = agent.select_action(obs, use_random=False)\n",
    "        obs, _, done, truncated, _ = env.step(action)\n",
    "        frame = grab_frame(env)\n",
    "        if frame is not None:\n",
    "            frames.append(frame)\n",
    "        steps += 1\n",
    "    env.close()\n",
    "    if not frames:\n",
    "        print(f\"⚠ フレーム未取得: {output_path}\")\n",
    "        return\n",
    "    imageio.mimsave(output_path, frames, fps=fps)\n",
    "    print(f\"✓ 動画保存: {output_path} (frames={len(frames)})\")\n",
    "\n",
    "\n",
    "for point in custom_omega_points:\n",
    "    param_dict = normalize_param_point(point)\n",
    "    param_tag = \"_\".join(f\"{k}-{param_dict[k]:.3f}\" for k in param_keys)\n",
    "    for seed in video_episode_seeds:\n",
    "        env = instantiate_render_env(param_dict)\n",
    "        timestamp = datetime.now().strftime(\"%Y%m%d-%H%M%S\")\n",
    "        video_name = f\"video_{video_method}_seed{seed}_{param_tag}_{timestamp}.mp4\"\n",
    "        output_path = video_output_dir / video_name\n",
    "        record_episode_video(env, video_agent, seed=seed, max_steps=video_max_steps, fps=video_fps, output_path=output_path, reset_options=param_dict)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [],
   "outputs": [],
   "execution_count": null
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "rrls311",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}